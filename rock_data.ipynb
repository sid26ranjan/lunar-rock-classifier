{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/siddhant/Desktop/data_sets/rock dataset/DataSet\n"
     ]
    }
   ],
   "source": [
    "cd /home/siddhant/Desktop/data_sets/rock dataset/DataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### import necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### loading the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir=\"/home/siddhant/Desktop/data_sets/rock dataset/DataSet/Train Images\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " there are two categorries of the rock define them as a list we will use it later\n",
    "\n",
    "defining the image size we will use in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories=['Large','Small']\n",
    "img_size=150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "importing the necessary modules from keras like dense(for defining the neural network),flatten (for converting \n",
    "\n",
    "layers into single arrays),conv2d (for defining convolution layer),maxpool2d(for pooling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Flatten, Conv2D, MaxPool2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### importing imagedatagenerator for image augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(16, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=RMSprop(lr=0.001), loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### defining the image augmentation part with the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11998 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "      rotation_range=40,\n",
    "      width_shift_range=0.2,\n",
    "      height_shift_range=0.2,\n",
    "      shear_range=0.2,\n",
    "      zoom_range=0.2,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')\n",
    "train_generator = train_datagen.flow_from_directory(train_dir,\n",
    "                                                    batch_size=100,\n",
    "                                                    class_mode='binary',\n",
    "                                                    target_size=(150, 150))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "120/120 [==============================] - 303s 3s/step - loss: 0.4623 - acc: 0.8159\n",
      "Epoch 2/20\n",
      "120/120 [==============================] - 211s 2s/step - loss: 0.1742 - acc: 0.9395\n",
      "Epoch 3/20\n",
      "120/120 [==============================] - 211s 2s/step - loss: 0.1042 - acc: 0.9670\n",
      "Epoch 4/20\n",
      "120/120 [==============================] - 216s 2s/step - loss: 0.0687 - acc: 0.9787\n",
      "Epoch 5/20\n",
      "120/120 [==============================] - 211s 2s/step - loss: 0.0563 - acc: 0.9827\n",
      "Epoch 6/20\n",
      "120/120 [==============================] - 212s 2s/step - loss: 0.0348 - acc: 0.9890\n",
      "Epoch 7/20\n",
      "120/120 [==============================] - 211s 2s/step - loss: 0.0431 - acc: 0.9879\n",
      "Epoch 8/20\n",
      "120/120 [==============================] - 211s 2s/step - loss: 0.0509 - acc: 0.9872\n",
      "Epoch 9/20\n",
      "120/120 [==============================] - 211s 2s/step - loss: 0.0439 - acc: 0.9887\n",
      "Epoch 10/20\n",
      "120/120 [==============================] - 212s 2s/step - loss: 0.0360 - acc: 0.9881\n",
      "Epoch 11/20\n",
      "120/120 [==============================] - 217s 2s/step - loss: 0.0476 - acc: 0.9902\n",
      "Epoch 12/20\n",
      "120/120 [==============================] - 213s 2s/step - loss: 0.0332 - acc: 0.9900\n",
      "Epoch 13/20\n",
      "120/120 [==============================] - 211s 2s/step - loss: 0.0283 - acc: 0.9912\n",
      "Epoch 14/20\n",
      "120/120 [==============================] - 212s 2s/step - loss: 0.0782 - acc: 0.9857\n",
      "Epoch 15/20\n",
      "120/120 [==============================] - 211s 2s/step - loss: 0.0733 - acc: 0.9869\n",
      "Epoch 16/20\n",
      "120/120 [==============================] - 211s 2s/step - loss: 0.1189 - acc: 0.9812\n",
      "Epoch 17/20\n",
      "120/120 [==============================] - 212s 2s/step - loss: 0.0255 - acc: 0.9911\n",
      "Epoch 18/20\n",
      "120/120 [==============================] - 211s 2s/step - loss: 0.0899 - acc: 0.9880\n",
      "Epoch 19/20\n",
      "120/120 [==============================] - 212s 2s/step - loss: 0.0300 - acc: 0.9909\n",
      "Epoch 20/20\n",
      "120/120 [==============================] - 211s 2s/step - loss: 0.0285 - acc: 0.9912\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(train_generator,\n",
    "                              epochs=20,\n",
    "                              verbose=1,\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Loading the test data for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('/home/siddhant/Desktop/data_sets/rock dataset/DataSet/test.csv')\n",
    "length_test = len(test['Image_File'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Loading images one by one and predicting there type and storing them in the test file in the class column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/siddhant/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/home/siddhant/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:190: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "testDF = pd.read_csv('/home/siddhant/Desktop/data_sets/rock dataset/DataSet/test.csv')\n",
    "\n",
    "for i in range(length_test):\n",
    "    img1 = image.load_img('/home/siddhant/Desktop/data_sets/rock dataset/DataSet/Test Images/'+ test['Image_File'][i], target_size=(150,150))\n",
    "    img = image.img_to_array(img1)\n",
    "    img = img / 255.0\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    prediction = model.predict(img, batch_size=None, steps=1)  \n",
    "    if (prediction[:, :] > 0.5):\n",
    "        testDF['Class'][i] = \"Small\"\n",
    "    else:\n",
    "        testDF['Class'][i] = \"Large\"\n",
    "    #print(i)\n",
    "#print(testDF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/siddhant/Desktop/data_sets/rock dataset\n"
     ]
    }
   ],
   "source": [
    "cd /home/siddhant/Desktop/data_sets/rock dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### saving the dataframe as a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDF.to_csv('third_hacker.csv',index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
